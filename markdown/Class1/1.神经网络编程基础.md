## 神经网络编程基础

为了能把训练集表示得更紧凑一点，定义一个矩阵用大写$X$的表示，它由输入向量$x^{(1)}$、$x^{(2)}$等组成，如下图放在矩阵的列中，把$x^{(1)}$作为第一列放在矩阵中，$x^{(2)}$作为第二列，$x^{(m)}$放到第$m$列，然后就得到了训练集矩阵$X$。所以这个矩阵有$m$列，然后这个矩阵的高度记为$n_x$，注意有时候可能因为其他某些原因，矩阵$X$会由训练样本按照行堆叠起来而不是列，如下图所示：$x^{(1)}$的转置直到$x^{(m)}$的转置，但是在实现神经网络的时候，**使用左边的这种形式，会让整个实现的过程变得更加简单**。

![数据集表示](http://www.ai-start.com/dl2017/images/55345ba411053da11ff843bbb3406369.png)

### 逻辑回归(Logistic Regression)

对于二元分类问题来讲，给定一个输入特征向量$X$，一个算法的预测结果称之为$\hat{y}$，也就是对实际值 $y$ 的估计。$X$是一个$n_x$维的向量（相当于有$n_x$个特征的特征向量）。

我们用$w$来表示逻辑回归的参数，这也是一个$n_x$维向量（因为$w$实际上是特征权重，维度与特征向量相同），参数里面还有实数$b$（表示偏差）。所以给出输入$x$以及参数$w$和$b$之后，让$\hat{y}={{w}^{T}}x+b$。

这时候我们得到的是一个关于输入$x$的线性函数，实际上这是在做线性回归时所用到的，但是这对于二元分类问题来讲不是一个非常好的算法。因此在逻辑回归中，我们的输出$\hat{y}$应该是等于由上面得到的线性函数式子作为自变量的**sigmoid**函数中，将线性函数转换为非线性函数。
$$
\begin{eqnarray*}
\sigma(z) &&=&& \frac{1}{1 + e^{-z}}  \\
\hat{y} &&=&& \sigma({{w}^{T}}x+b) 
\end{eqnarray*}
$$
![sigmoid函数](http://www.ai-start.com/dl2017/images/7e304debcca5945a3443d56bcbdd2964.png)

在继续进行下一步之前，介绍一种符号惯例，可以让参数$w$和参数$b$分开。在符号上要注意的一点是**当我们对神经网络进行编程时经常会让参数$w$和参数$b$分开，在这里参数$b$对应的是一种偏置**。

在之前的机器学习课程里，可能已经见过处理这个问题时的其他符号表示。比如在某些例子里，定义一个额外的特征称之为${{x}_{0}}$，并且使它等于1，那么现在$X$就是一个$n_x + 1$维的变量，然后定义$\hat{y}=\sigma \left( {{\theta }^{T}}x \right)$的**sigmoid**函数。在这个备选的符号惯例里，有一个参数向量${{\theta }_{0}},{{\theta }_{1}},{{\theta }_{2}},...,{{\theta }_{{{n}_{x}}}}$，这样${{\theta }_{0}}$就充当了$b$，这是一个实数，而剩下的${{\theta }_{1}}$ 直到${{\theta }_{{{n}_{x}}}}$充当了$w$。

当你实现你的神经网络时，有一个比较简单的方法是保持$b$和$w$分开。

### 逻辑回归的代价函数（Logistic Regression Cost Function）

为了让模型通过学习调整参数，需要给予一个$m$样本的训练集，这会让你在训练集上找到合适的参数$w$和参数$b$，来得到你的输出。

![逻辑回归函数](http://www.ai-start.com/dl2017/images/4c9a27b071ce9162dbbcdad3393061d2.png)

对训练集的预测值，我们将它写成$\hat{y}$，我们更希望它会接近于训练集中的$y$值，为了对上面的公式更详细的介绍，我们需要说明上面的定义是对一个训练样本来说的，这种形式也使用于每个训练样本，我们使用这些带有圆括号的上标来区分索引和样本，训练样本$i$所对应的预测值是${{y}^{(i)}}$,是用训练样本的${{w}^{T}}{{x}^{(i)}}+b$然后通过**sigmoid**函数来得到，也可以把$z$定义为${{z}^{(i)}}={{w}^{T}}{{x}^{(i)}}+b$,我们将使用上标$(i)$来指明数据表示$x$或者$y$或者$z$或者其他数据的第$i$个训练样本，这就是上标$(i)$的含义。

**损失函数：**

损失函数又叫做误差函数，用来衡量算法的运行情况，**Loss function:$L\left( \hat{y},y \right)$.**

我们通过这个$L$称为的损失函数，来衡量预测输出值和实际值有多接近。==一般我们用预测值和实际值的平方差或者它们平方差的一半，但是通常在逻辑回归中我们不这么做，因为当我们在学习逻辑回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部最优值，梯度下降法很可能找不到全局最优值== 。虽然平方差是一个不错的损失函数，但是我们在逻辑回归模型中会定义另外一个损失函数。

我们在逻辑回归中用到的损失函数是：
$$
L\left( \hat{y},y \right)=-y\log(\hat{y})-(1-y)\log (1-\hat{y})
$$
为什么要用这个函数作为逻辑损失函数？当我们使用平方误差作为损失函数的时候，会想要让这个误差尽可能地小。对于这个逻辑回归损失函数，我们也想让它尽可能地小，为了更好地理解这个损失函数怎么起作用，我们举两个例子：

- 当$y=1$时损失函数$L=-\log (\hat{y})$，如果想要损失函数$L$尽可能得小，那么$\hat{y}$就要尽可能大，因为**sigmoid**函数取值$[0,1]$，所以$\hat{y}$会无限接近于1。
- 当$y=0$时损失函数$L=-\log (1-\hat{y})$，如果想要损失函数$L$尽可能得小，那么$\hat{y}$就要尽可能小，因为**sigmoid**函数取值$[0,1]$，所以$\hat{y}$会无限接近于0。

**在这门课中有很多的函数效果和现在这个类似，就是如果$y$等于1，我们就尽可能让$\hat{y}$变大，如果$y$等于0，我们就尽可能让 $\hat{y}$ 变小。**

**代价函数：**

损失函数是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，算法的代价函数是对$m$个样本的损失函数求和然后除以$m$
$$
J\left( w,b \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{L\left( {{{\hat{y}}}^{(i)}},{{y}^{(i)}} \right)}=\frac{1}{m}\sum\limits_{i=1}^{m}{\left( -{{y}^{(i)}}\log {{{\hat{y}}}^{(i)}}-(1-{{y}^{(i)}})\log (1-{{{\hat{y}}}^{(i)}}) \right)}
$$
损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价，所以==在训练逻辑回归模型时候，我们需要找到合适的$w$和$b$，来让代价函数 $J$ 的总代价降到最低== 。根据我们对逻辑回归算法的推导及对单个样本的损失函数的推导和针对算法所选用参数的总代价函数的推导，结果表明逻辑回归可以看做是一个非常小的神经网络。

### 梯度下降法（Gradient Descent）

![梯度下降说明](http://www.ai-start.com/dl2017/images/c5eda5608fd2f4d846559ed8e89ed33c.jpg)

如图，代价函数是一个凸函数(**convex function**)，像一个大碗一样。最低点就是代价函数的最小值点。假定代价函数只有一个参数:
$$
\begin{eqnarray*}
&& Repeat\{ && \\
&& && w := w - \alpha\frac{dJ(w)}{dw} \\
&& \} &&
\end{eqnarray*}
$$
$a $ 表示学习率（**learning rate**），用来控制步长（**step**），即向下走一步的长度。$\frac{dJ(w)}{dw}$  就是函数$J(w)$对$w$ 求导（**derivative**）。逻辑回归的代价函数是含有两个参数的：
$$
\begin{eqnarray*}
w && := && w - \alpha\frac{\partial J(w,b)}{\partial w} \\
b && := && b - \alpha\frac{\partial J(w,b)}{\partial b} 
\end{eqnarray*}
$$
$\partial $ 表示求偏导符号，可以读作round，$\frac{\partial J(w,b)}{\partial w}$  就是函数$J(w,b)$ 对$w$ 求偏导，在代码中我们会使用$dw$ 表示这个结果，$\frac{\partial J(w,b)}{\partial b}$  就是函数$J(w,b)$对$b$ 求偏导，在代码中我们会使用$db$ 表示这个结果，小写字母$d$ 用在求导数（**derivative**），即函数只有一个参数，偏导数符号$\partial $ 用在求偏导（**partial derivative**），即函数含有两个以上的参数。

### 图的导数计算（Derivatives with a Computation Graph）



