## 浅层神经网络

神经网络看起来是下面这个样子。可以把许多**sigmoid**单元堆叠起来形成一个神经网络。它包含了之前讲的计算的两个步骤：首先计算出值$z$，然后通过$\sigma(z)$计算值$a$。

![浅层神经网络](http://www.ai-start.com/dl2017/images/L1_week3_4.png)

这里使用符号$^{[m]}$表示第$m$层网络（层数是从0开始，即$m=0$）。这样可以保证$^{[m]}$不会和之前用来表示第$i$个训练样本的$^{(i)}$混淆。

### 神经网络的表示（Neural Network Representation）

![神经网络](http://www.ai-start.com/dl2017/images/L1_week3_3.png)

输入特征$x_1$、$x_2$、$x_3$，它们被竖直地堆叠起来，这叫做神经网络的**输入层**。它包含了神经网络的输入；然后这里有另外一层称之为**隐藏层**。在本例中最后一层只由一个结点构成，而这个只有一个结点的层被称为**输出层**，它负责产生预测值。

解释隐藏层的含义：在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入$x$也包含了目标输出$y$，所以术语隐藏层的含义是在训练集中，这些中间结点的准确值我们是不知道到的，也就是说你看不见它们在训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，在训练集中你是无法看到的。所以这也解释了词语隐藏层，只是表示你无法在训练集中看到他们。

现在再引入几个符号，就像之前用向量$x$表示输入特征。这里有个可代替的记号==$a^{[0]}$可以用来表示输入特征==。$a$表示激活的意思，它意味着网络中不同层的值会传递到它们后面的层中，输入层将$x$传递给隐藏层，所以将输入层的激活值称为$a^{[0]}$；下一层即隐藏层也同样会产生一些激活值，将其记作$a^{[1]}$，所以具体地，这里的第一个单元或结点将其表示为$a^{[1]}_{1}$，第二个结点的值我们记为$a^{[1]}_{2}$以此类推。所以这里的是一个四维的向量如果写成Python代码，那么它是一个规模为$4\times1$的矩阵或一个大小为4的列向量，也称为四个隐藏层单元。

在约定俗成的符号传统中，在这里所看到的这个例子，只能叫做一个两层的神经网络。原因是当我们**计算网络的层数时，输入层是不算入总层数内**，所以隐藏层是第一层，输出层是第二层。第二个惯例是我们将输入层称为第零层，所以在技术上，这仍然是一个三层的神经网络，因为这里有输入层、隐藏层，还有输出层。但是在传统的符号使用中，如果你阅读研究论文或者在这门课中，你会看到人们将这个神经网络称为一个两层的神经网络，因为我们不将输入层看作一个标准的层。

最后，我们要看到的隐藏层以及最后的输出层是带有参数的，这里的隐藏层将拥有两个参数$W$和$b$，给它们加上上标$^{[1]}$($W^{[1]}$,$b^{[1]}$)，表示这些参数是和第一层这个隐藏层有关系的。在这个例子中我们会看到$W$是一个$4\times3$的矩阵，而$b$是一个$4\times1$的向量，第一个数字4源自于我们需要计算四个结点或隐藏层单元，然后数字3源自于这里有三个输入特征。相似的输出层也有一些与之关联的参数$W^{[2]}$以及$b^{[2]}$。从维数上来看，它们的规模分别是$1\times4$以及$1\times1$。

### 计算一个神经网络的输出（Computing a Neural Network's output）

![神经网络神经单元计算示意](http://www.ai-start.com/dl2017/images/L1_week3_6.png)

关于神经网络是怎么计算的，从之前提及的逻辑回归开始，如上图所示。用圆圈表示神经网络的计算单元，逻辑回归的计算有两个步骤，首先按步骤计算出$z$，然后在第二步中以**sigmoid**函数为激活函数计算$z$（得出$a$），一个神经网络只是这样子做了好多次重复计算。
$$
\left[
		\begin{array}{c}
		z^{[1]}_{1}\\
		z^{[1]}_{2}\\
		z^{[1]}_{3}\\
		z^{[1]}_{4}\\
		\end{array}
		\right]
		 =
	\overbrace{
	\left[
		\begin{array}{c}
		...W^{[1]T}_{1}...\\
		...W^{[1]T}_{2}...\\
		...W^{[1]T}_{3}...\\
		...W^{[1]T}_{4}...
		\end{array}
		\right]
		}^{W^{[1]}}
		*
	\overbrace{
	\left[
		\begin{array}{c}
		x_1\\
		x_2\\
		x_3\\
		\end{array}
		\right]
		}^{input}
		+
	\overbrace{
	\left[
		\begin{array}{c}
		b^{[1]}_1\\
		b^{[1]}_2\\
		b^{[1]}_3\\
		b^{[1]}_4\\
		\end{array}
		\right]
		}^{b^{[1]}}
$$
向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，例如隐藏层中的$w$纵向堆积起来变成一个$(4,3)$的矩阵，用符号$W^{[1]}$表示。
$$
a^{[1]} =
	\left[
		\begin{array}{c}
		a^{[1]}_{1}\\
		a^{[1]}_{2}\\
		a^{[1]}_{3}\\
		a^{[1]}_{4}
		\end{array}
		\right]
		= \sigma(z^{[1]})
$$
对于神经网络的第一层，给予一个输入$x$，得到$a^{[1]}$，$x$可以表示为$a^{[0]}$。通过相似的衍生后会发现，后一层的表示同样可以写成类似的形式，得到$a^{[2]}$，$\hat{y} = a^{[2]}$ 。

![神经网络向量化实现](http://www.ai-start.com/dl2017/images/L1_week3_7.png)

当有一个包含一层隐藏层的神经网络，需要去实现以计算得到输出的是右边的四个等式，并且可以看成是一个向量化的计算过程，计算出隐藏层的四个逻辑回归单元和整个隐藏层的输出结果，如果编程实现需要的也只是这四行代码。

### 多样本向量化（Vectorizing across multiple examples）

上一节给出了如何计算出$z^{[1]}$，$a^{[1]}$，$z^{[2]}$，$a^{[2]}$。

对于一个给定的输入特征向量$X$，这四个等式可以计算出$\alpha^{[2]}$等于$\hat{y}$。这是针对于单一的训练样本。如果有$m$个训练样本,那么就需要重复这个过程。

用第一个训练样本$x^{(1)}$来计算出预测值$\hat{y}^{(1)}$，就是第一个训练样本上得出的结果。

然后，用$x^{(2)}$来计算出预测值$\hat{y}^{(2)}$，循环往复，直至用$x^{(m)}$计算出$\hat{y}^{(m)}$。

如果有一个非向量化形式的实现，而且要计算出它的预测值，对于所有训练样本，需要让$i$从1到$m$实现这四个等式：

$z^{[1](i)}=W^{[1](i)}x^{(i)}+b^{[1](i)}$

$a^{[1](i)}=\sigma(z^{[1](i)})$

$z^{[2](i)}=W^{[2](i)}a^{[1](i)}+b^{[2](i)}$

$a^{[2](i)}=\sigma(z^{[2](i)})$

对于上面的这个方程中的$^{(i)}$，是所有依赖于训练样本数的变量，即将$(i)$添加到$x$，$z$和$a$。如果想计算$m$个训练样本上的所有输出，就应该向量化整个计算，以简化这列:
$$
x =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
		\right]
$$

$$
W^{[1]} = 
\left[
\begin{array}{c}
		...W^{[1]T}_{1}...\\
		...W^{[1]T}_{2}...\\
		...W^{[1]T}_{3}...\\
		...W^{[1]T}_{4}...
\end{array}
\right]
$$

$$
A^{[1]} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		a^{[1](1)} & a^{[1](2)} & \cdots & a^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
		\right]
$$

现在 $W^{[1]}$ 是一个矩阵，$x^{(1)},x^{(2)},x^{(3)}$都是列向量，矩阵乘以列向量得到列向量，下面将它们用图形直观的表示出来:
$$
W^{[1]}  x =
		\left[
                  \begin{array}{c}
                          ...W^{[1]T}_{1}...\\
                          ...W^{[1]T}_{2}...\\
                          ...W^{[1]T}_{3}...\\
                          ...W^{[1]T}_{4}...
                  \end{array}
                  \right]
		
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		x^{(1)} & x^{(2)} & x^{(3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		w^{(1)}x^{(1)} & w^{(1)}x^{(2)} & w^{(1)}x^{(3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=\\
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		z^{[1](1)} & z^{[1](2)} & z^{[1](3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=
		Z^{[1]}
$$
之前对单个样本的计算写成$z^{[1](i)} = W^{[1]}x^{(i)} + b^{[1]}$ ，使用这种形式，因为当有不同的训练样本时，将它们堆到矩阵$X$的各列中，那么它们的输出也就会相应的堆叠到矩阵 $Z^{[1]}$ 的各列中。现在就可以直接计算矩阵 $Z^{[1]}$ 加上$b^{[1]}$，因为列向量 $b^{[1]}$ 和矩阵 $Z^{[1]}$的列向量有着相同的尺寸，而**Python**的广播机制对于这种矩阵与向量直接相加的处理方式是，将向量与矩阵的每一列相加。
所以这一节只是说明了为什么公式 $Z^{[1]} =W^{[1]}X + \ b^{[1]}$是前向传播的第一步计算的正确向量化实现，但事实证明，类似的分析可以发现，前向传播的其它步也可以使用非常相似的逻辑，即如果将输入按列向量横向堆叠进矩阵，那么通过公式计算之后，也能得到成列堆叠的输出。

$$
\left.
		\begin{array}{r}
		\text{$z^{[1](i)} = W^{[1](i)}x^{(i)} + b^{[1]}$}\\
		\text{$\alpha^{[1](i)} = \sigma(z^{[1](i)})$}\\
		\text{$z^{[2](i)} = W^{[2](i)}\alpha^{[1](i)} + b^{[2]}$}\\
		\text{$\alpha^{[2](i)} = \sigma(z^{[2](i)})$}\\
		\end{array}
		\right\}
		\implies
		\begin{cases}
		\text{$A^{[1]} = \sigma(z^{[1]})$}\\
		\text{$z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$}\\ 
		\text{$A^{[2]} = \sigma(z^{[2]})$}\\ 
		\end{cases}
$$
从水平上看，矩阵$A$代表了各个训练样本。从竖直上看，矩阵$A$的不同的索引对应于不同的隐藏单元。对于矩阵$Z，X$情况也类似，水平方向上，对应于不同的训练样本；竖直方向上，对应不同的输入特征，而这就是神经网络输入层中各个节点。

使用向量化的方法，可以不需要显示循环，而直接通过矩阵运算从$X$就可以计算出 $A^{[1]}$，实际上$X$可以记为 $A^{[0]}$，使用同样的方法就可以由神经网络中的每一层的输入 $A^{[i-1]}$ 计算输出 $A^{[i]}$。其实这些方程有一定对称性，其中第一个方程也可以写成$Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]}$ 。这里我们有一个双层神经网络，后续会看到随着网络的深度变大，基本上也还是重复这两步运算，只不过是比这里的重复次数更多。

以上就是对神经网络向量化实现的正确性的解释，到目前为止，我们仅使用**sigmoid**函数作为激活函数，事实上这并非最好的选择，后续会深入的讲解如何使用更多不同种类的激活函数。

### 激活函数（Activation functions）



