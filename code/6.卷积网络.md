[TOC]

## 卷积函数

### Zero Padding

通过Padding，可以继续保持图片大小，它可以帮助我们将更多的信息保存在图像的边界。如果没有填充，下一层的少数值将受到边缘像素的影响。填充数$p = \frac{f - 1}{2}$。

```python
def zero_pad(X, pad):
    """
    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, 
    as illustrated in Figure 1.
    
    Argument:
    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images
    pad -- integer, amount of padding around each image on vertical and horizontal dimensions
    
    Returns:
    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)
    """
    
    X_pad = np.pad(X, ((0,0), (pad, pad), (pad, pad),(0,0)), 'constant')
    
    return X_pad
```



### Convolve window

![](https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/%E5%8D%B7%E7%A7%AF%E7%A4%BA%E6%84%8F%E5%9B%BE.gif)

```python
def conv_single_step(a_slice_prev, W, b):
    """
    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation 
    of the previous layer.
    
    Arguments:
    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)
    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)
    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)
    
    Returns:
    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data
    """

    # Element-wise product between a_slice and W.
    s = a_slice_prev * W
    # Sum over all entries of the volume s.
    Z = np.sum(s)
    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.
    Z = Z + float(b)

    return Z
```



### Convolution forward

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/%E5%8D%B7%E7%A7%AF%E5%88%87%E7%89%87.png" style="width:400px;height:300px;">

卷积后的输出维度：
$$
\begin{split}
n_H = \lfloor \frac{n_{H_{prev}} + 2 \times pad}{stride} - f \rfloor +1 \\
n_W = \lfloor \frac{n_{W_{prev}} + 2 \times pad}{stride} - f \rfloor +1
\end{split}
$$

```python
def conv_forward(A_prev, W, b, hparameters):
    """
    Implements the forward propagation for a convolution function
    
    Arguments:
    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)
    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C) 这里的W最后一维指明了过滤器个数
    b -- Biases, numpy array of shape (1, 1, 1, n_C) b一样，最后一维是过滤器个数
    hparameters -- python dictionary containing "stride" and "pad"
        
    Returns:
    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)
    cache -- cache of values needed for the conv_backward() function
    """
    
    # Retrieve dimensions from A_prev's shape (≈1 line)  
    (m, n_H_prev, n_W_prev, n_C_prev) = np.shape(A_prev)
    
    # Retrieve dimensions from W's shape (≈1 line)
    # f x f 大小，通道数是n_C_prev，过滤器个数是n_C
    (f, f, n_C_prev, n_C) = np.shape(W)
    
    # Retrieve information from "hparameters" (≈2 lines)
    stride = hparameters["stride"] # 步长
    pad = hparameters["pad"] # 填充数
    
    # 计算卷积后的维度
    n_H = int((n_H_prev + 2 * pad - f)/stride) + 1 # int()向下取整
    n_W = int((n_W_prev + 2 * pad - f)/stride) + 1
    
    # Initialize the output volume Z with zeros. (≈1 line)
    Z = np.zeros(shape=(m, n_H, n_W, n_C))
    
    # 对网络进行padding
    A_prev_pad = zero_pad(A_prev, pad)
    
    for i in range(m):
        a_prev_pad = A_prev_pad[i] # 依次选取样本
        # 遍历计算卷积后网络中的值
        for h in range(n_H): 
            for w in range(n_W): 
                for c in range(n_C):           
                    # 当前卷积的起始位置
                    vert_start = h * stride
                    vert_end = vert_start + f
                    horiz_start = w * stride
                    horiz_end = horiz_start + f         
                    
                    # 当前要卷积的切片
                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]
                    # 对当前切片进行卷积，W和b只选当前c通道的
                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])                                   
    
    # Making sure your output shape is correct
    assert(Z.shape == (m, n_H, n_W, n_C))
    
    # Save information in "cache" for the backprop
    cache = (A_prev, W, b, hparameters)
    
    return Z, cache
```



### Convolution backward 



## 卷积池化

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96.png" style="width:500px;height:300px;">

### Pooling forward

### Create mask

### Distribute value

### Pooling backward 

